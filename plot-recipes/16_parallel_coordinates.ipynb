{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1b2faf",
   "metadata": {},
   "source": [
    "# Parallel Coordinates Plot - Multivariate Data Analysis\n",
    "\n",
    "**Use Case**: Visualize high-dimensional data, compare multiple variables simultaneously, identify patterns and clusters\n",
    "\n",
    "This notebook demonstrates how to create effective parallel coordinates plots for analyzing relationships across multiple continuous variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from pandas.plotting import parallel_coordinates\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Parallel coordinates plot visualization libraries loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets for parallel coordinates\n",
    "# 1. Student Performance Dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "n_students = 200\n",
    "student_data = []\n",
    "\n",
    "for i in range(n_students):\n",
    "    # Create correlated performance metrics\n",
    "    base_ability = np.random.normal(75, 15)  # Base academic ability\n",
    "    \n",
    "    # Subject scores with different correlations to base ability\n",
    "    math_score = max(0, min(100, base_ability + np.random.normal(0, 8)))\n",
    "    science_score = max(0, min(100, base_ability * 0.9 + np.random.normal(0, 10)))\n",
    "    english_score = max(0, min(100, base_ability * 0.7 + np.random.normal(0, 12)))\n",
    "    history_score = max(0, min(100, base_ability * 0.6 + np.random.normal(0, 15)))\n",
    "    \n",
    "    # Other metrics\n",
    "    study_hours = max(0, np.random.normal(25, 8))  # Hours per week\n",
    "    attendance = max(50, min(100, base_ability * 0.3 + np.random.normal(70, 10)))\n",
    "    extracurricular = np.random.randint(0, 8)  # Number of activities\n",
    "    \n",
    "    # Performance level based on average scores\n",
    "    avg_score = (math_score + science_score + english_score + history_score) / 4\n",
    "    if avg_score >= 85:\n",
    "        performance_level = 'Excellent'\n",
    "    elif avg_score >= 70:\n",
    "        performance_level = 'Good'\n",
    "    elif avg_score >= 55:\n",
    "        performance_level = 'Average'\n",
    "    else:\n",
    "        performance_level = 'Below Average'\n",
    "    \n",
    "    student_data.append({\n",
    "        'Student_ID': f'S{i+1:03d}',\n",
    "        'Math_Score': round(math_score, 1),\n",
    "        'Science_Score': round(science_score, 1),\n",
    "        'English_Score': round(english_score, 1),\n",
    "        'History_Score': round(history_score, 1),\n",
    "        'Study_Hours_Weekly': round(study_hours, 1),\n",
    "        'Attendance_Percent': round(attendance, 1),\n",
    "        'Extracurricular_Count': extracurricular,\n",
    "        'Performance_Level': performance_level\n",
    "    })\n",
    "\n",
    "student_df = pd.DataFrame(student_data)\n",
    "\n",
    "# 2. Employee Performance Dataset\n",
    "n_employees = 150\n",
    "\n",
    "employee_data = []\n",
    "departments = ['Engineering', 'Sales', 'Marketing', 'HR', 'Finance']\n",
    "experience_levels = ['Junior', 'Mid', 'Senior', 'Lead']\n",
    "\n",
    "for i in range(n_employees):\n",
    "    dept = np.random.choice(departments)\n",
    "    exp_level = np.random.choice(experience_levels)\n",
    "    \n",
    "    # Base performance varies by experience\n",
    "    exp_multiplier = {'Junior': 0.7, 'Mid': 1.0, 'Senior': 1.3, 'Lead': 1.5}[exp_level]\n",
    "    base_performance = np.random.normal(7, 1.5) * exp_multiplier\n",
    "    \n",
    "    # Department-specific variations\n",
    "    dept_bonus = {'Engineering': 0.5, 'Sales': 0.3, 'Marketing': 0.2, 'HR': 0.1, 'Finance': 0.4}[dept]\n",
    "    \n",
    "    technical_skills = max(1, min(10, base_performance + dept_bonus + np.random.normal(0, 1)))\n",
    "    communication = max(1, min(10, base_performance * 0.8 + np.random.normal(0, 1.2)))\n",
    "    leadership = max(1, min(10, base_performance * 0.6 + np.random.normal(0, 1.5)))\n",
    "    problem_solving = max(1, min(10, base_performance * 0.9 + np.random.normal(0, 1)))\n",
    "    productivity = max(1, min(10, base_performance * 1.1 + np.random.normal(0, 0.8)))\n",
    "    \n",
    "    # Salary based on performance and experience\n",
    "    salary = (base_performance * 8000 + \n",
    "             {'Junior': 45000, 'Mid': 65000, 'Senior': 85000, 'Lead': 110000}[exp_level] +\n",
    "             np.random.normal(0, 5000))\n",
    "    \n",
    "    employee_data.append({\n",
    "        'Employee_ID': f'E{i+1:03d}',\n",
    "        'Department': dept,\n",
    "        'Experience_Level': exp_level,\n",
    "        'Technical_Skills': round(technical_skills, 1),\n",
    "        'Communication': round(communication, 1),\n",
    "        'Leadership': round(leadership, 1),\n",
    "        'Problem_Solving': round(problem_solving, 1),\n",
    "        'Productivity': round(productivity, 1),\n",
    "        'Salary': int(salary)\n",
    "    })\n",
    "\n",
    "employee_df = pd.DataFrame(employee_data)\n",
    "\n",
    "# 3. Car Specifications Dataset\n",
    "n_cars = 180\n",
    "\n",
    "car_data = []\n",
    "car_types = ['Sedan', 'SUV', 'Hatchback', 'Coupe', 'Truck']\n",
    "fuel_types = ['Gasoline', 'Hybrid', 'Electric', 'Diesel']\n",
    "\n",
    "for i in range(n_cars):\n",
    "    car_type = np.random.choice(car_types)\n",
    "    fuel_type = np.random.choice(fuel_types)\n",
    "    \n",
    "    # Base specifications vary by car type\n",
    "    if car_type == 'Sedan':\n",
    "        base_hp = np.random.normal(200, 40)\n",
    "        base_mpg = np.random.normal(28, 5)\n",
    "        base_price = np.random.normal(35000, 8000)\n",
    "    elif car_type == 'SUV':\n",
    "        base_hp = np.random.normal(280, 50)\n",
    "        base_mpg = np.random.normal(22, 4)\n",
    "        base_price = np.random.normal(45000, 12000)\n",
    "    elif car_type == 'Hatchback':\n",
    "        base_hp = np.random.normal(150, 30)\n",
    "        base_mpg = np.random.normal(32, 6)\n",
    "        base_price = np.random.normal(25000, 5000)\n",
    "    elif car_type == 'Coupe':\n",
    "        base_hp = np.random.normal(320, 80)\n",
    "        base_mpg = np.random.normal(20, 4)\n",
    "        base_price = np.random.normal(50000, 15000)\n",
    "    else:  # Truck\n",
    "        base_hp = np.random.normal(350, 60)\n",
    "        base_mpg = np.random.normal(18, 3)\n",
    "        base_price = np.random.normal(40000, 10000)\n",
    "    \n",
    "    # Fuel type adjustments\n",
    "    if fuel_type == 'Hybrid':\n",
    "        mpg_bonus = 1.5\n",
    "        price_bonus = 3000\n",
    "    elif fuel_type == 'Electric':\n",
    "        mpg_bonus = 3.0  # MPGe\n",
    "        price_bonus = 8000\n",
    "    elif fuel_type == 'Diesel':\n",
    "        mpg_bonus = 1.2\n",
    "        price_bonus = 2000\n",
    "    else:  # Gasoline\n",
    "        mpg_bonus = 1.0\n",
    "        price_bonus = 0\n",
    "    \n",
    "    horsepower = max(100, base_hp + np.random.normal(0, 20))\n",
    "    mpg = max(10, base_mpg * mpg_bonus + np.random.normal(0, 3))\n",
    "    price = max(15000, base_price + price_bonus + np.random.normal(0, 3000))\n",
    "    \n",
    "    # Other specs\n",
    "    engine_size = max(1.0, horsepower / 100 + np.random.normal(0, 0.3))\n",
    "    safety_rating = np.random.uniform(3.5, 5.0)\n",
    "    \n",
    "    car_data.append({\n",
    "        'Car_ID': f'C{i+1:03d}',\n",
    "        'Car_Type': car_type,\n",
    "        'Fuel_Type': fuel_type,\n",
    "        'Horsepower': round(horsepower),\n",
    "        'MPG': round(mpg, 1),\n",
    "        'Engine_Size_L': round(engine_size, 1),\n",
    "        'Safety_Rating': round(safety_rating, 1),\n",
    "        'Price_USD': int(price)\n",
    "    })\n",
    "\n",
    "car_df = pd.DataFrame(car_data)\n",
    "\n",
    "# 4. Wine Quality Dataset (using sklearn wine dataset as base)\n",
    "wine_sklearn = load_wine()\n",
    "wine_features = pd.DataFrame(wine_sklearn.data, columns=wine_sklearn.feature_names)\n",
    "wine_features['quality_class'] = wine_sklearn.target\n",
    "wine_features['quality_name'] = wine_features['quality_class'].map({0: 'Low', 1: 'Medium', 2: 'High'})\n",
    "\n",
    "# Select key features for visualization\n",
    "wine_df = wine_features[['alcohol', 'total_phenols', 'flavanoids', 'color_intensity', \n",
    "                        'od280/od315_of_diluted_wines', 'proline', 'quality_name']].copy()\n",
    "wine_df.columns = ['Alcohol', 'Phenols', 'Flavanoids', 'Color_Intensity', \n",
    "                   'OD_Ratio', 'Proline', 'Quality']\n",
    "\n",
    "# 5. City Livability Index\n",
    "n_cities = 120\n",
    "\n",
    "city_data = []\n",
    "regions = ['North America', 'Europe', 'Asia Pacific', 'Latin America', 'Middle East & Africa']\n",
    "\n",
    "for i in range(n_cities):\n",
    "    region = np.random.choice(regions)\n",
    "    \n",
    "    # Regional baseline differences\n",
    "    regional_baseline = {\n",
    "        'North America': 75,\n",
    "        'Europe': 80,\n",
    "        'Asia Pacific': 70,\n",
    "        'Latin America': 65,\n",
    "        'Middle East & Africa': 60\n",
    "    }[region]\n",
    "    \n",
    "    base_score = np.random.normal(regional_baseline, 10)\n",
    "    \n",
    "    # Correlated livability factors\n",
    "    cost_of_living = max(30, min(100, 100 - base_score * 0.5 + np.random.normal(0, 15)))  # Inverse correlation\n",
    "    safety = max(20, min(100, base_score * 0.8 + np.random.normal(0, 12)))\n",
    "    healthcare = max(40, min(100, base_score * 0.9 + np.random.normal(0, 10)))\n",
    "    education = max(50, min(100, base_score * 0.7 + np.random.normal(0, 15)))\n",
    "    infrastructure = max(30, min(100, base_score * 0.8 + np.random.normal(0, 12)))\n",
    "    environment = max(20, min(100, base_score * 0.6 + np.random.normal(0, 18)))\n",
    "    culture = max(40, min(100, base_score * 0.5 + np.random.normal(0, 20)))\n",
    "    \n",
    "    # Overall livability score\n",
    "    overall = (safety + healthcare + education + infrastructure + environment + culture) / 6\n",
    "    \n",
    "    city_data.append({\n",
    "        'City_ID': f'City_{i+1:03d}',\n",
    "        'Region': region,\n",
    "        'Cost_of_Living': round(cost_of_living, 1),\n",
    "        'Safety': round(safety, 1),\n",
    "        'Healthcare': round(healthcare, 1),\n",
    "        'Education': round(education, 1),\n",
    "        'Infrastructure': round(infrastructure, 1),\n",
    "        'Environment': round(environment, 1),\n",
    "        'Culture': round(culture, 1),\n",
    "        'Overall_Score': round(overall, 1)\n",
    "    })\n",
    "\n",
    "city_df = pd.DataFrame(city_data)\n",
    "\n",
    "print(\"Sample parallel coordinates datasets created:\")\n",
    "print(f\"Student Performance: {len(student_df)} students with 8 metrics\")\n",
    "print(f\"Employee Performance: {len(employee_df)} employees with 6 skills + salary\")  \n",
    "print(f\"Car Specifications: {len(car_df)} cars with 6 technical specs\")\n",
    "print(f\"Wine Quality: {len(wine_df)} wines with 6 chemical properties\")\n",
    "print(f\"City Livability: {len(city_df)} cities with 8 livability factors\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample Student Data:\")\n",
    "print(student_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic parallel coordinates plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "fig.suptitle('Parallel Coordinates Visualizations - Multivariate Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Student Performance Analysis\n",
    "ax1 = axes[0, 0]\n",
    "\n",
    "# Prepare data for parallel coordinates\n",
    "student_numeric = student_df[['Math_Score', 'Science_Score', 'English_Score', 'History_Score', \n",
    "                             'Study_Hours_Weekly', 'Attendance_Percent', 'Extracurricular_Count']].copy()\n",
    "student_with_class = student_numeric.copy()\n",
    "student_with_class['Performance_Level'] = student_df['Performance_Level']\n",
    "\n",
    "# Use pandas parallel_coordinates\n",
    "parallel_coordinates(student_with_class, 'Performance_Level', ax=ax1, alpha=0.6)\n",
    "ax1.set_title('Student Performance Metrics', fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 2. Employee Skills Analysis  \n",
    "ax2 = axes[0, 1]\n",
    "\n",
    "employee_numeric = employee_df[['Technical_Skills', 'Communication', 'Leadership', \n",
    "                               'Problem_Solving', 'Productivity']].copy()\n",
    "employee_with_dept = employee_numeric.copy()\n",
    "employee_with_dept['Department'] = employee_df['Department']\n",
    "\n",
    "parallel_coordinates(employee_with_dept, 'Department', ax=ax2, alpha=0.6)\n",
    "ax2.set_title('Employee Skills by Department', fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 3. Car Specifications\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "# Normalize car data for better visualization\n",
    "car_numeric = car_df[['Horsepower', 'MPG', 'Engine_Size_L', 'Safety_Rating']].copy()\n",
    "# Scale price to 0-100 range for visualization\n",
    "car_numeric['Price_Scaled'] = (car_df['Price_USD'] - car_df['Price_USD'].min()) / (car_df['Price_USD'].max() - car_df['Price_USD'].min()) * 100\n",
    "\n",
    "car_with_type = car_numeric.copy()\n",
    "car_with_type['Car_Type'] = car_df['Car_Type']\n",
    "\n",
    "parallel_coordinates(car_with_type, 'Car_Type', ax=ax3, alpha=0.6)\n",
    "ax3.set_title('Car Specifications by Type', fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 4. Wine Quality Analysis\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "wine_for_plot = wine_df.copy()\n",
    "parallel_coordinates(wine_for_plot, 'Quality', ax=ax4, alpha=0.6)\n",
    "ax4.set_title('Wine Chemical Properties by Quality', fontweight='bold')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab35ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced parallel coordinates techniques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "fig.suptitle('Advanced Parallel Coordinates Techniques', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Normalized Parallel Coordinates with Brushing Effect\n",
    "ax1 = axes[0, 0]\n",
    "\n",
    "# Standardize city data for better comparison\n",
    "city_numeric = city_df[['Cost_of_Living', 'Safety', 'Healthcare', 'Education', \n",
    "                       'Infrastructure', 'Environment', 'Culture']].copy()\n",
    "\n",
    "# Normalize to 0-1 scale\n",
    "scaler = MinMaxScaler()\n",
    "city_normalized = pd.DataFrame(scaler.fit_transform(city_numeric), \n",
    "                              columns=city_numeric.columns)\n",
    "city_normalized['Region'] = city_df['Region']\n",
    "\n",
    "# Create custom parallel coordinates with highlighting\n",
    "regions = city_normalized['Region'].unique()\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(regions)))\n",
    "\n",
    "for i, region in enumerate(regions):\n",
    "    region_data = city_normalized[city_normalized['Region'] == region]\n",
    "    \n",
    "    # Plot background lines (all other regions) in light gray\n",
    "    other_data = city_normalized[city_normalized['Region'] != region]\n",
    "    for idx, row in other_data.iterrows():\n",
    "        values = row[city_numeric.columns].values\n",
    "        ax1.plot(range(len(city_numeric.columns)), values, \n",
    "                color='lightgray', alpha=0.1, linewidth=0.5)\n",
    "    \n",
    "    # Plot current region in color\n",
    "    for idx, row in region_data.iterrows():\n",
    "        values = row[city_numeric.columns].values\n",
    "        ax1.plot(range(len(city_numeric.columns)), values, \n",
    "                color=colors[i], alpha=0.7, linewidth=1.5, \n",
    "                label=region if idx == region_data.index[0] else \"\")\n",
    "\n",
    "ax1.set_xticks(range(len(city_numeric.columns)))\n",
    "ax1.set_xticklabels(city_numeric.columns, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Normalized Score (0-1)')\n",
    "ax1.set_title('City Livability Factors by Region\\n(With Background Context)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Percentile-based Parallel Coordinates\n",
    "ax2 = axes[0, 1]\n",
    "\n",
    "# Convert student scores to percentiles\n",
    "student_percentiles = student_df[['Math_Score', 'Science_Score', 'English_Score', 'History_Score']].copy()\n",
    "\n",
    "for col in student_percentiles.columns:\n",
    "    student_percentiles[col] = student_percentiles[col].rank(pct=True) * 100\n",
    "\n",
    "student_percentiles['Performance_Level'] = student_df['Performance_Level']\n",
    "\n",
    "# Plot with percentile interpretation\n",
    "parallel_coordinates(student_percentiles, 'Performance_Level', ax=ax2, alpha=0.6)\n",
    "ax2.set_title('Student Performance (Percentile Rankings)', fontweight='bold')\n",
    "ax2.set_ylabel('Percentile Rank')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Add percentile reference lines\n",
    "for pct in [25, 50, 75]:\n",
    "    ax2.axhline(y=pct, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax2.text(-0.1, pct, f'{pct}th', ha='right', va='center', fontsize=8)\n",
    "\n",
    "# 3. Multi-scale Parallel Coordinates\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "# Employee data with different scales\n",
    "employee_multi = employee_df[['Technical_Skills', 'Communication', 'Leadership', \n",
    "                             'Problem_Solving', 'Productivity']].copy()\n",
    "\n",
    "# Add salary scaled to 1-10 range to match other metrics\n",
    "employee_multi['Salary_Scaled'] = ((employee_df['Salary'] - employee_df['Salary'].min()) / \n",
    "                                  (employee_df['Salary'].max() - employee_df['Salary'].min()) * 9) + 1\n",
    "\n",
    "employee_multi['Experience_Level'] = employee_df['Experience_Level']\n",
    "\n",
    "parallel_coordinates(employee_multi, 'Experience_Level', ax=ax3, alpha=0.6)\n",
    "ax3.set_title('Employee Metrics by Experience Level\\n(Salary Scaled to 1-10)', fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 4. Correlation-based Parallel Coordinates  \n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Show high-correlation vs low-correlation variables\n",
    "wine_corr = wine_df[['Alcohol', 'Phenols', 'Flavanoids', 'Color_Intensity', 'OD_Ratio', 'Proline']].copy()\n",
    "\n",
    "# Calculate correlations with alcohol (first variable)\n",
    "correlations = wine_corr.corr()['Alcohol'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Variable correlations with Alcohol:\")\n",
    "for var, corr in correlations.items():\n",
    "    print(f\"  {var}: {corr:.3f}\")\n",
    "\n",
    "# Reorder columns by correlation strength\n",
    "ordered_cols = correlations.index.tolist()\n",
    "wine_ordered = wine_corr[ordered_cols].copy()\n",
    "wine_ordered['Quality'] = wine_df['Quality']\n",
    "\n",
    "parallel_coordinates(wine_ordered, 'Quality', ax=ax4, alpha=0.6)\n",
    "ax4.set_title('Wine Properties (Ordered by Correlation)\\nAlcohol → Most Correlated', fontweight='bold')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e1507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive parallel coordinates examples (Plotly structure)\n",
    "print(\"Interactive Parallel Coordinates (Plotly):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Basic Interactive Parallel Coordinates\")\n",
    "print(\"Code structure:\")\n",
    "print(\"\"\"\n",
    "fig = go.Figure(data=\n",
    "    go.Parcoords(\n",
    "        line=dict(color=df['target_column'],\n",
    "                 colorscale='Viridis',\n",
    "                 showscale=True),\n",
    "        dimensions=list([\n",
    "            dict(range=[df[col].min(), df[col].max()],\n",
    "                 constraintrange=[df[col].quantile(0.2), df[col].quantile(0.8)],\n",
    "                 label=col, values=df[col])\n",
    "            for col in numeric_columns\n",
    "        ])\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(title='Interactive Parallel Coordinates Plot')\n",
    "fig.show()\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n2. Parallel Coordinates with Brushing and Linking\")\n",
    "print(\"Code structure:\")\n",
    "print(\"\"\"\n",
    "# Create figure with brushing capability\n",
    "fig = go.Figure(data=\n",
    "    go.Parcoords(\n",
    "        line=dict(color=df['category_col'],\n",
    "                 colorscale='Set1',\n",
    "                 showscale=True,\n",
    "                 colorbar=dict(title=\"Category\")),\n",
    "        dimensions=[\n",
    "            dict(range=[min_val, max_val],\n",
    "                 constraintrange=[filter_min, filter_max],  # Enable brushing\n",
    "                 label=label, \n",
    "                 values=values,\n",
    "                 tickvals=tick_positions,\n",
    "                 ticktext=tick_labels) \n",
    "            for each dimension\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add selection and filtering capabilities\n",
    "fig.update_layout(\n",
    "    title=\"Brushable Parallel Coordinates\",\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n3. Multi-dimensional Comparison Dashboard\")\n",
    "print(\"Code structure:\")\n",
    "print(\"\"\"\n",
    "# Create subplots for different views\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('All Data', 'Filtered View', 'Correlation Matrix', 'Distribution'),\n",
    "    specs=[[{\"type\": \"parcoords\"}, {\"type\": \"parcoords\"}],\n",
    "           [{\"type\": \"heatmap\"}, {\"type\": \"histogram\"}]]\n",
    ")\n",
    "\n",
    "# Add parallel coordinates plots\n",
    "fig.add_trace(go.Parcoords(...), row=1, col=1)\n",
    "fig.add_trace(go.Parcoords(...), row=1, col=2)\n",
    "\n",
    "# Add correlation heatmap\n",
    "fig.add_trace(go.Heatmap(...), row=2, col=1)\n",
    "\n",
    "# Add distribution plot\n",
    "fig.add_trace(go.Histogram(...), row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n4. Categorical and Continuous Mixed Dimensions\")\n",
    "print(\"Code structure:\")\n",
    "print(\"\"\"\n",
    "# Handle mixed data types\n",
    "dimensions = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in ['object', 'category']:\n",
    "        # Categorical dimension\n",
    "        unique_vals = df[col].unique()\n",
    "        dimensions.append(dict(\n",
    "            label=col,\n",
    "            values=df[col].map({v: i for i, v in enumerate(unique_vals)}),\n",
    "            tickvals=list(range(len(unique_vals))),\n",
    "            ticktext=unique_vals\n",
    "        ))\n",
    "    else:\n",
    "        # Continuous dimension\n",
    "        dimensions.append(dict(\n",
    "            label=col,\n",
    "            values=df[col],\n",
    "            range=[df[col].min(), df[col].max()]\n",
    "        ))\n",
    "\n",
    "fig = go.Figure(data=go.Parcoords(\n",
    "    line=dict(color=color_values),\n",
    "    dimensions=dimensions\n",
    "))\n",
    "\n",
    "fig.show()\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde802e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering analysis with parallel coordinates\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Clustering Analysis with Parallel Coordinates:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. K-means clustering on student data\n",
    "print(\"1. STUDENT PERFORMANCE CLUSTERING:\")\n",
    "\n",
    "# Prepare student data for clustering\n",
    "student_cluster_data = student_df[['Math_Score', 'Science_Score', 'English_Score', 'History_Score', \n",
    "                                  'Study_Hours_Weekly', 'Attendance_Percent']].copy()\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "student_scaled = scaler.fit_transform(student_cluster_data)\n",
    "\n",
    "# Perform k-means clustering\n",
    "kmeans_student = KMeans(n_clusters=4, random_state=42)\n",
    "student_clusters = kmeans_student.fit_predict(student_scaled)\n",
    "\n",
    "student_cluster_data['Cluster'] = student_clusters\n",
    "student_cluster_data['Cluster_Name'] = student_cluster_data['Cluster'].map({\n",
    "    0: 'High Achievers', 1: 'Balanced Students', 2: 'Struggling Students', 3: 'Subject Specialists'\n",
    "})\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "print(\"   Cluster Analysis:\")\n",
    "for cluster_id in range(4):\n",
    "    cluster_data = student_cluster_data[student_cluster_data['Cluster'] == cluster_id]\n",
    "    cluster_name = cluster_data['Cluster_Name'].iloc[0]\n",
    "    print(f\"\\n   {cluster_name} (n={len(cluster_data)}):\")\n",
    "    \n",
    "    for col in ['Math_Score', 'Science_Score', 'English_Score', 'History_Score']:\n",
    "        mean_score = cluster_data[col].mean()\n",
    "        print(f\"     {col}: {mean_score:.1f}\")\n",
    "    \n",
    "    print(f\"     Study Hours: {cluster_data['Study_Hours_Weekly'].mean():.1f}\")\n",
    "    print(f\"     Attendance: {cluster_data['Attendance_Percent'].mean():.1f}%\")\n",
    "\n",
    "# 2. Employee clustering\n",
    "print(f\"\\n2. EMPLOYEE SKILLS CLUSTERING:\")\n",
    "\n",
    "employee_cluster_data = employee_df[['Technical_Skills', 'Communication', 'Leadership', \n",
    "                                   'Problem_Solving', 'Productivity']].copy()\n",
    "\n",
    "employee_scaled = scaler.fit_transform(employee_cluster_data)\n",
    "kmeans_employee = KMeans(n_clusters=3, random_state=42)\n",
    "employee_clusters = kmeans_employee.fit_predict(employee_scaled)\n",
    "\n",
    "employee_cluster_data['Cluster'] = employee_clusters\n",
    "employee_cluster_data['Department'] = employee_df['Department']\n",
    "employee_cluster_data['Experience_Level'] = employee_df['Experience_Level']\n",
    "\n",
    "cluster_names = {0: 'Technical Specialists', 1: 'Well-Rounded', 2: 'Leadership Focused'}\n",
    "employee_cluster_data['Cluster_Name'] = employee_cluster_data['Cluster'].map(cluster_names)\n",
    "\n",
    "print(\"   Employee Cluster Characteristics:\")\n",
    "for cluster_id in range(3):\n",
    "    cluster_data = employee_cluster_data[employee_cluster_data['Cluster'] == cluster_id]\n",
    "    cluster_name = cluster_names[cluster_id]\n",
    "    print(f\"\\n   {cluster_name} (n={len(cluster_data)}):\")\n",
    "    \n",
    "    for skill in ['Technical_Skills', 'Communication', 'Leadership', 'Problem_Solving', 'Productivity']:\n",
    "        mean_skill = cluster_data[skill].mean()\n",
    "        print(f\"     {skill}: {mean_skill:.1f}\")\n",
    "    \n",
    "    # Department distribution\n",
    "    dept_dist = cluster_data['Department'].value_counts()\n",
    "    print(f\"     Top Departments: {dept_dist.head(2).to_dict()}\")\n",
    "\n",
    "# Visualize clusters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "fig.suptitle('Clustering Analysis with Parallel Coordinates', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Student clusters\n",
    "ax1 = axes[0, 0]\n",
    "student_for_plot = student_cluster_data[['Math_Score', 'Science_Score', 'English_Score', \n",
    "                                        'History_Score', 'Study_Hours_Weekly', 'Attendance_Percent', \n",
    "                                        'Cluster_Name']].copy()\n",
    "parallel_coordinates(student_for_plot, 'Cluster_Name', ax=ax1, alpha=0.6)\n",
    "ax1.set_title('Student Performance Clusters', fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 2. Employee clusters\n",
    "ax2 = axes[0, 1]\n",
    "employee_for_plot = employee_cluster_data[['Technical_Skills', 'Communication', 'Leadership', \n",
    "                                          'Problem_Solving', 'Productivity', 'Cluster_Name']].copy()\n",
    "parallel_coordinates(employee_for_plot, 'Cluster_Name', ax=ax2, alpha=0.6)\n",
    "ax2.set_title('Employee Skills Clusters', fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 3. City clustering\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "city_cluster_data = city_df[['Safety', 'Healthcare', 'Education', 'Infrastructure', \n",
    "                            'Environment', 'Culture']].copy()\n",
    "city_scaled = scaler.fit_transform(city_cluster_data)\n",
    "kmeans_city = KMeans(n_clusters=3, random_state=42)\n",
    "city_clusters = kmeans_city.fit_predict(city_scaled)\n",
    "\n",
    "city_cluster_data['Cluster'] = city_clusters\n",
    "city_cluster_names = {0: 'Developed Cities', 1: 'Emerging Cities', 2: 'Developing Cities'}\n",
    "city_cluster_data['Cluster_Name'] = city_cluster_data['Cluster'].map(city_cluster_names)\n",
    "\n",
    "city_for_plot = city_cluster_data.copy()\n",
    "parallel_coordinates(city_for_plot, 'Cluster_Name', ax=ax3, alpha=0.6)\n",
    "ax3.set_title('City Livability Clusters', fontweight='bold')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 4. Cluster comparison matrix\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Create a cluster comparison for students\n",
    "cluster_means = student_cluster_data.groupby('Cluster_Name')[['Math_Score', 'Science_Score', \n",
    "                                                             'English_Score', 'History_Score']].mean()\n",
    "\n",
    "# Normalize for heatmap\n",
    "cluster_means_norm = (cluster_means - cluster_means.min()) / (cluster_means.max() - cluster_means.min())\n",
    "\n",
    "im = ax4.imshow(cluster_means_norm.values, cmap='RdYlBu_r', aspect='auto')\n",
    "ax4.set_xticks(range(len(cluster_means.columns)))\n",
    "ax4.set_xticklabels(cluster_means.columns, rotation=45, ha='right')\n",
    "ax4.set_yticks(range(len(cluster_means.index)))\n",
    "ax4.set_yticklabels(cluster_means.index)\n",
    "ax4.set_title('Student Cluster Profiles\\n(Normalized Scores)', fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(cluster_means.index)):\n",
    "    for j in range(len(cluster_means.columns)):\n",
    "        text = ax4.text(j, i, f'{cluster_means.iloc[i, j]:.1f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax4, shrink=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of parallel coordinates data\n",
    "print(\"Parallel Coordinates Statistical Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Correlation Analysis\n",
    "print(\"1. CORRELATION ANALYSIS:\")\n",
    "\n",
    "datasets_for_corr = {\n",
    "    'Student Performance': student_df[['Math_Score', 'Science_Score', 'English_Score', 'History_Score', \n",
    "                                      'Study_Hours_Weekly', 'Attendance_Percent']],\n",
    "    'Employee Skills': employee_df[['Technical_Skills', 'Communication', 'Leadership', \n",
    "                                   'Problem_Solving', 'Productivity']],\n",
    "    'Wine Properties': wine_df[['Alcohol', 'Phenols', 'Flavanoids', 'Color_Intensity', 'OD_Ratio', 'Proline']],\n",
    "    'City Livability': city_df[['Safety', 'Healthcare', 'Education', 'Infrastructure', 'Environment', 'Culture']]\n",
    "}\n",
    "\n",
    "for dataset_name, data in datasets_for_corr.items():\n",
    "    print(f\"\\n   {dataset_name} Correlations:\")\n",
    "    corr_matrix = data.corr()\n",
    "    \n",
    "    # Find highest correlations (excluding self-correlations)\n",
    "    corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_value = corr_matrix.iloc[i, j]\n",
    "            corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], abs(corr_value), corr_value))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    corr_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(\"     Strongest correlations:\")\n",
    "    for var1, var2, abs_corr, corr in corr_pairs[:3]:\n",
    "        print(f\"       {var1} ↔ {var2}: {corr:+.3f}\")\n",
    "    \n",
    "    print(\"     Weakest correlations:\")\n",
    "    for var1, var2, abs_corr, corr in corr_pairs[-3:]:\n",
    "        print(f\"       {var1} ↔ {var2}: {corr:+.3f}\")\n",
    "\n",
    "# 2. Dimensionality and Complexity Analysis\n",
    "print(f\"\\n2. DIMENSIONALITY ANALYSIS:\")\n",
    "\n",
    "for dataset_name, data in datasets_for_corr.items():\n",
    "    n_vars = len(data.columns)\n",
    "    n_observations = len(data)\n",
    "    \n",
    "    # Calculate effective dimensionality using PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_data)\n",
    "    \n",
    "    # Find number of components for 95% variance\n",
    "    cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "    \n",
    "    print(f\"\\n   {dataset_name}:\")\n",
    "    print(f\"     Total Variables: {n_vars}\")\n",
    "    print(f\"     Observations: {n_observations}\")\n",
    "    print(f\"     Ratio (obs/vars): {n_observations/n_vars:.1f}\")\n",
    "    print(f\"     Components for 95% variance: {n_components_95}\")\n",
    "    print(f\"     First 3 PC variance: {cumsum_var[2]:.1%}\")\n",
    "\n",
    "# 3. Pattern Detection Analysis\n",
    "print(f\"\\n3. PATTERN DETECTION ANALYSIS:\")\n",
    "\n",
    "# Analyze student performance patterns\n",
    "student_analysis = student_df.copy()\n",
    "\n",
    "# Define performance categories\n",
    "def categorize_performance(row):\n",
    "    scores = [row['Math_Score'], row['Science_Score'], row['English_Score'], row['History_Score']]\n",
    "    avg_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    \n",
    "    if std_score < 5:  # Consistent performance\n",
    "        if avg_score >= 80:\n",
    "            return 'Consistent High'\n",
    "        elif avg_score >= 60:\n",
    "            return 'Consistent Medium'\n",
    "        else:\n",
    "            return 'Consistent Low'\n",
    "    else:  # Variable performance\n",
    "        if max(scores) - min(scores) > 20:\n",
    "            return 'Highly Variable'\n",
    "        else:\n",
    "            return 'Moderately Variable'\n",
    "\n",
    "student_analysis['Performance_Pattern'] = student_analysis.apply(categorize_performance, axis=1)\n",
    "\n",
    "print(\"   Student Performance Patterns:\")\n",
    "pattern_counts = student_analysis['Performance_Pattern'].value_counts()\n",
    "for pattern, count in pattern_counts.items():\n",
    "    percentage = (count / len(student_analysis)) * 100\n",
    "    print(f\"     {pattern}: {count} students ({percentage:.1f}%)\")\n",
    "\n",
    "# Analyze relationship between patterns and study habits\n",
    "print(\"\\n   Pattern vs Study Habits:\")\n",
    "for pattern in pattern_counts.index:\n",
    "    pattern_data = student_analysis[student_analysis['Performance_Pattern'] == pattern]\n",
    "    avg_study = pattern_data['Study_Hours_Weekly'].mean()\n",
    "    avg_attendance = pattern_data['Attendance_Percent'].mean()\n",
    "    print(f\"     {pattern}: {avg_study:.1f} hrs/week, {avg_attendance:.1f}% attendance\")\n",
    "\n",
    "# 4. Outlier Detection in Multivariate Space\n",
    "print(f\"\\n4. OUTLIER DETECTION:\")\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "datasets_for_outliers = {\n",
    "    'Students': student_df[['Math_Score', 'Science_Score', 'English_Score', 'History_Score']],\n",
    "    'Employees': employee_df[['Technical_Skills', 'Communication', 'Leadership', 'Problem_Solving']],\n",
    "    'Cars': car_df[['Horsepower', 'MPG', 'Engine_Size_L', 'Safety_Rating']],\n",
    "    'Cities': city_df[['Safety', 'Healthcare', 'Education', 'Infrastructure']]\n",
    "}\n",
    "\n",
    "for dataset_name, data in datasets_for_outliers.items():\n",
    "    # Detect outliers using Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "    outliers = iso_forest.fit_predict(data)\n",
    "    \n",
    "    n_outliers = np.sum(outliers == -1)\n",
    "    outlier_percentage = (n_outliers / len(data)) * 100\n",
    "    \n",
    "    print(f\"\\n   {dataset_name} Outliers:\")\n",
    "    print(f\"     Detected outliers: {n_outliers} ({outlier_percentage:.1f}%)\")\n",
    "    \n",
    "    if n_outliers > 0:\n",
    "        # Analyze outlier characteristics\n",
    "        outlier_indices = np.where(outliers == -1)[0]\n",
    "        outlier_data = data.iloc[outlier_indices]\n",
    "        normal_data = data.iloc[np.where(outliers == 1)[0]]\n",
    "        \n",
    "        print(\"     Outlier characteristics (vs normal):\")\n",
    "        for col in data.columns:\n",
    "            outlier_mean = outlier_data[col].mean()\n",
    "            normal_mean = normal_data[col].mean()\n",
    "            diff = outlier_mean - normal_mean\n",
    "            print(f\"       {col}: {outlier_mean:.1f} vs {normal_mean:.1f} ({diff:+.1f})\")\n",
    "\n",
    "# 5. Visualization Effectiveness Analysis\n",
    "print(f\"\\n5. VISUALIZATION EFFECTIVENESS:\")\n",
    "\n",
    "print(\"   Parallel Coordinates Suitability Analysis:\")\n",
    "\n",
    "for dataset_name, data in datasets_for_corr.items():\n",
    "    n_vars = len(data.columns)\n",
    "    n_obs = len(data)\n",
    "    \n",
    "    # Calculate visualization complexity score\n",
    "    corr_matrix = data.corr().abs()\n",
    "    avg_correlation = (corr_matrix.sum().sum() - len(corr_matrix)) / (len(corr_matrix) * (len(corr_matrix) - 1))\n",
    "    \n",
    "    # Assess data characteristics\n",
    "    if n_vars <= 5:\n",
    "        complexity = \"Simple\"\n",
    "    elif n_vars <= 10:\n",
    "        complexity = \"Moderate\"\n",
    "    else:\n",
    "        complexity = \"Complex\"\n",
    "    \n",
    "    if avg_correlation > 0.7:\n",
    "        correlation_level = \"High\"\n",
    "    elif avg_correlation > 0.4:\n",
    "        correlation_level = \"Moderate\"\n",
    "    else:\n",
    "        correlation_level = \"Low\"\n",
    "    \n",
    "    # Effectiveness recommendation\n",
    "    if n_vars <= 8 and avg_correlation > 0.3:\n",
    "        effectiveness = \"Highly Effective\"\n",
    "    elif n_vars <= 12 and avg_correlation > 0.2:\n",
    "        effectiveness = \"Moderately Effective\"\n",
    "    else:\n",
    "        effectiveness = \"Consider Alternatives\"\n",
    "    \n",
    "    print(f\"\\n   {dataset_name}:\")\n",
    "    print(f\"     Variables: {n_vars} ({complexity})\")\n",
    "    print(f\"     Avg Correlation: {avg_correlation:.3f} ({correlation_level})\")\n",
    "    print(f\"     Effectiveness: {effectiveness}\")\n",
    "\n",
    "print(f\"\\n6. BEST PRACTICES FOR PARALLEL COORDINATES:\")\n",
    "\n",
    "print(\"   ✓ Optimal for 3-12 continuous variables\")\n",
    "print(\"   ✓ Most effective with moderate correlations (0.3-0.7)\")\n",
    "print(\"   ✓ Normalize/standardize variables for comparison\")\n",
    "print(\"   ✓ Order variables by correlation or logical flow\")\n",
    "print(\"   ✓ Use color to encode categories or clusters\")\n",
    "print(\"   ✓ Consider brushing for interactive exploration\")\n",
    "print(\"   ✓ Limit line opacity to reduce overplotting\")\n",
    "\n",
    "print(f\"\\nWhen to Use Parallel Coordinates:\")\n",
    "print(\"   • Multivariate data exploration\")\n",
    "print(\"   • Identifying patterns and clusters\")\n",
    "print(\"   • Comparing groups across multiple dimensions\")\n",
    "print(\"   • Quality control and anomaly detection\")\n",
    "print(\"   • Feature selection and correlation analysis\")\n",
    "\n",
    "print(f\"\\nAlternatives to Consider:\")\n",
    "print(\"   • Scatterplot matrices for pairwise relationships\")\n",
    "print(\"   • Radar/spider charts for profile comparison\")\n",
    "print(\"   • Heatmaps for correlation visualization\")\n",
    "print(\"   • PCA plots for dimensionality reduction\")\n",
    "print(\"   • t-SNE/UMAP for non-linear structure\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
