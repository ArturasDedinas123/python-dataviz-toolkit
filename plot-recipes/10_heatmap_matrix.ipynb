{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e01e85",
   "metadata": {},
   "source": [
    "# Heatmap - Matrix Relationships\n",
    "\n",
    "**Use Case**: Show matrix relationships (correlation matrix, confusion matrix, schedule grid)\n",
    "\n",
    "This notebook demonstrates how to create effective heatmaps for visualizing relationships in matrix data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643e32b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_classification\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import sklearn for classification examples\n",
    "try:\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    SKLEARN_AVAILABLE = True\n",
    "    print(\"‚úÖ scikit-learn imported successfully\")\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  scikit-learn not available - confusion matrix example will be simulated\")\n",
    "    print(\"   Install with: pip install scikit-learn\")\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for different heatmap types\n",
    "# 1. Correlation matrix data\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Create correlated financial metrics\n",
    "interest_rate = np.random.uniform(2, 6, n_samples)\n",
    "inflation = interest_rate * 0.7 + np.random.normal(0, 0.5, n_samples)\n",
    "unemployment = 8 - interest_rate * 0.8 + np.random.normal(0, 0.8, n_samples)\n",
    "gdp_growth = 4 - unemployment * 0.3 + np.random.normal(0, 0.5, n_samples)\n",
    "stock_return = gdp_growth * 1.5 + np.random.normal(0, 2, n_samples)\n",
    "\n",
    "economic_data = pd.DataFrame({\n",
    "    'Interest_Rate': interest_rate,\n",
    "    'Inflation': inflation,\n",
    "    'Unemployment': unemployment,\n",
    "    'GDP_Growth': gdp_growth,\n",
    "    'Stock_Return': stock_return\n",
    "})\n",
    "\n",
    "# 2. Schedule/Calendar data\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "hours = ['9 AM', '10 AM', '11 AM', '12 PM', '1 PM', '2 PM', '3 PM', '4 PM']\n",
    "schedule_data = np.random.choice([0, 1, 2, 3], size=(len(hours), len(days)), p=[0.3, 0.4, 0.2, 0.1])\n",
    "schedule_df = pd.DataFrame(schedule_data, index=hours, columns=days)\n",
    "\n",
    "# 3. Sales performance data\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "products = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n",
    "sales_matrix = np.random.uniform(50, 200, size=(len(products), len(regions)))\n",
    "sales_df = pd.DataFrame(sales_matrix, index=products, columns=regions)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Correlation heatmap\n",
    "correlation_matrix = economic_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0,\n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8}, ax=ax1)\n",
    "ax1.set_title('Economic Indicators Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Schedule heatmap\n",
    "schedule_labels = ['Free', 'Meeting', 'Busy', 'Blocked']\n",
    "sns.heatmap(schedule_df, annot=True, cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Activity Level'}, ax=ax2)\n",
    "ax2.set_title('Weekly Schedule Heatmap', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Day of Week')\n",
    "ax2.set_ylabel('Time')\n",
    "\n",
    "# 3. Sales performance heatmap\n",
    "sns.heatmap(sales_df, annot=True, cmap='Greens', fmt='.0f',\n",
    "            cbar_kws={'label': 'Sales ($K)'}, ax=ax3)\n",
    "ax3.set_title('Sales Performance by Product and Region', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Region')\n",
    "ax3.set_ylabel('Product')\n",
    "\n",
    "# 4. Confusion matrix heatmap\n",
    "if SKLEARN_AVAILABLE:\n",
    "    # Generate classification data\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_classes=4, \n",
    "                              n_informative=8, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train classifier\n",
    "    clf = RandomForestClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    class_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3']\n",
    "else:\n",
    "    # Create simulated confusion matrix when sklearn is not available\n",
    "    print(\"üìä Simulating confusion matrix (sklearn not available)\")\n",
    "    cm = np.array([[45, 3, 2, 1],\n",
    "                   [4, 52, 3, 0], \n",
    "                   [2, 1, 48, 4],\n",
    "                   [1, 0, 2, 47]])\n",
    "    class_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3']\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax4)\n",
    "ax4.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Predicted Label')\n",
    "ax4.set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced heatmap techniques\n",
    "# Generate more complex datasets\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Customer behavior data\n",
    "customers = [f'Customer_{i+1}' for i in range(20)]\n",
    "products_behavior = ['Website', 'Mobile_App', 'Email', 'Social_Media', 'Store']\n",
    "behavior_data = np.random.poisson(5, size=(len(customers), len(products_behavior)))\n",
    "behavior_df = pd.DataFrame(behavior_data, index=customers, columns=products_behavior)\n",
    "\n",
    "# 2. Gene expression-like data (clustered)\n",
    "genes = [f'Gene_{i+1}' for i in range(50)]\n",
    "conditions = ['Control', 'Treatment_A', 'Treatment_B', 'Treatment_C']\n",
    "expression_data = np.random.lognormal(0, 1, size=(len(genes), len(conditions)))\n",
    "expression_df = pd.DataFrame(expression_data, index=genes, columns=conditions)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Customer behavior with custom colors\n",
    "sns.heatmap(behavior_df, cmap='viridis', cbar_kws={'label': 'Interaction Count'}, ax=ax1)\n",
    "ax1.set_title('Customer Interaction Heatmap', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Channel')\n",
    "ax1.set_ylabel('Customer')\n",
    "\n",
    "# 2. Clustermap (hierarchical clustering)\n",
    "cluster_plot = sns.clustermap(expression_df, cmap='RdYlBu_r', figsize=(8, 10),\n",
    "                             cbar_kws={'label': 'Expression Level'})\n",
    "plt.setp(cluster_plot.ax_heatmap.get_xticklabels(), rotation=45, ha='right')\n",
    "cluster_plot.fig.suptitle('Gene Expression Clustermap', y=0.95, fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. Diverging heatmap (centered at 0)\n",
    "# Normalize sales data to show performance relative to average\n",
    "sales_normalized = (sales_df - sales_df.mean()) / sales_df.std()\n",
    "sns.heatmap(sales_normalized, annot=True, cmap='RdBu_r', center=0, fmt='.1f',\n",
    "            cbar_kws={'label': 'Std Deviations from Mean'}, ax=ax2)\n",
    "ax2.set_title('Normalized Sales Performance', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Region')\n",
    "ax2.set_ylabel('Product')\n",
    "\n",
    "# 4. Masked heatmap (hide certain values)\n",
    "# Create attendance data with some missing values\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']\n",
    "employees = [f'Employee_{i+1}' for i in range(15)]\n",
    "attendance_data = np.random.uniform(70, 100, size=(len(employees), len(months)))\n",
    "# Add some missing data (NaN)\n",
    "mask_indices = np.random.choice(range(len(employees) * len(months)), \n",
    "                               size=20, replace=False)\n",
    "attendance_flat = attendance_data.flatten()\n",
    "attendance_flat[mask_indices] = np.nan\n",
    "attendance_data = attendance_flat.reshape(attendance_data.shape)\n",
    "attendance_df = pd.DataFrame(attendance_data, index=employees, columns=months)\n",
    "\n",
    "# Create mask for missing values\n",
    "mask = attendance_df.isnull()\n",
    "sns.heatmap(attendance_df, mask=mask, annot=True, fmt='.0f', cmap='YlGn',\n",
    "            cbar_kws={'label': 'Attendance %'}, ax=ax3)\n",
    "ax3.set_title('Employee Attendance (Missing Data Masked)', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Month')\n",
    "ax3.set_ylabel('Employee')\n",
    "\n",
    "# 5. Multiple heatmaps comparison\n",
    "fig2, (ax5, ax6) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Before and after comparison\n",
    "sales_before = sales_df.copy()\n",
    "sales_after = sales_df * np.random.uniform(0.8, 1.4, size=sales_df.shape)\n",
    "\n",
    "sns.heatmap(sales_before, annot=True, fmt='.0f', cmap='Reds',\n",
    "            cbar_kws={'label': 'Sales Before ($K)'}, ax=ax5)\n",
    "ax5.set_title('Sales Before Campaign', fontsize=14, fontweight='bold')\n",
    "\n",
    "sns.heatmap(sales_after, annot=True, fmt='.0f', cmap='Greens',\n",
    "            cbar_kws={'label': 'Sales After ($K)'}, ax=ax6)\n",
    "ax6.set_title('Sales After Campaign', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis and insights\n",
    "print(\"Heatmap Analysis Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"Economic Indicators Correlation Analysis:\")\n",
    "strongest_correlations = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.5:  # Strong correlation threshold\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            strongest_correlations.append((var1, var2, corr_value))\n",
    "\n",
    "strongest_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "for var1, var2, corr in strongest_correlations:\n",
    "    direction = \"positive\" if corr > 0 else \"negative\"\n",
    "    strength = \"very strong\" if abs(corr) > 0.8 else \"strong\"\n",
    "    print(f\"  {var1} ‚Üî {var2}: {corr:.3f} ({strength} {direction})\")\n",
    "\n",
    "# Sales performance analysis\n",
    "print(f\"\\nSales Performance Analysis:\")\n",
    "best_region = sales_df.sum().idxmax()\n",
    "worst_region = sales_df.sum().idxmin()\n",
    "best_product = sales_df.sum(axis=1).idxmax()\n",
    "worst_product = sales_df.sum(axis=1).idxmin()\n",
    "\n",
    "print(f\"  Best Region: {best_region} (${sales_df.sum()[best_region]:.0f}K total)\")\n",
    "print(f\"  Worst Region: {worst_region} (${sales_df.sum()[worst_region]:.0f}K total)\")\n",
    "print(f\"  Best Product: {best_product} (${sales_df.sum(axis=1)[best_product]:.0f}K total)\")\n",
    "print(f\"  Worst Product: {worst_product} (${sales_df.sum(axis=1)[worst_product]:.0f}K total)\")\n",
    "\n",
    "# Find the best product-region combination\n",
    "max_cell = sales_df.max().max()\n",
    "max_location = sales_df.stack().idxmax()\n",
    "print(f\"  Best Combination: {max_location[0]} in {max_location[1]} (${max_cell:.0f}K)\")\n",
    "\n",
    "# Confusion matrix analysis\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"\\nConfusion Matrix Analysis:\")\n",
    "print(f\"  Overall Accuracy: {accuracy:.3f}\")\n",
    "if not SKLEARN_AVAILABLE:\n",
    "    print(\"  (Using simulated data - sklearn not available)\")\n",
    "\n",
    "# Per-class precision and recall\n",
    "for i, class_name in enumerate(class_names):\n",
    "    precision = cm[i, i] / np.sum(cm[:, i]) if np.sum(cm[:, i]) > 0 else 0\n",
    "    recall = cm[i, i] / np.sum(cm[i, :]) if np.sum(cm[i, :]) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f\"  {class_name}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "# Schedule analysis\n",
    "print(f\"\\nSchedule Analysis:\")\n",
    "busiest_day = schedule_df.sum().idxmax()\n",
    "busiest_hour = schedule_df.sum(axis=1).idxmax()\n",
    "print(f\"  Busiest Day: {busiest_day} ({schedule_df.sum()[busiest_day]} total activity)\")\n",
    "print(f\"  Busiest Hour: {busiest_hour} ({schedule_df.sum(axis=1)[busiest_hour]} total activity)\")\n",
    "\n",
    "print(f\"\\nHeatmap Best Practices:\")\n",
    "print(\"‚úì Use appropriate color schemes (diverging for correlation, sequential for magnitude)\")\n",
    "print(\"‚úì Include annotations for small matrices (< 10x10)\")\n",
    "print(\"‚úì Consider clustering for large datasets to reveal patterns\")\n",
    "print(\"‚úì Use masks to handle missing data appropriately\")\n",
    "print(\"‚úó Avoid rainbow color schemes that can be misleading\")\n",
    "\n",
    "if not SKLEARN_AVAILABLE:\n",
    "    print(f\"\\nüì¶ Optional Dependencies:\")\n",
    "    print(\"   For advanced classification examples, install scikit-learn:\")\n",
    "    print(\"   pip install scikit-learn\")\n",
    "    print(\"   conda install scikit-learn\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
